version: '3.8'

# ====================================== AIRFLOW ENVIRONMENT VARIABLES =======================================
x-environment: &airflow_environment
  AIRFLOW__API__AUTH_BACKEND: airflow.api.auth.backend.basic_auth
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CORE__STORE_DAG_CODE: 'true'
  AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
  AIRFLOW_CONN_LOCALS3: s3://user:password@?host=http%3A%2F%2Flocals3%3A9000
  AIRFLOW_CONN_INSIDE_AIRBNB: postgres://insideairbnb:insideairbnb@insideairbnb:5432/insideairbnb

x-airflow-image: &airflow_image apache/airflow:2.1.3-python3.8
# ====================================== /AIRFLOW ENVIRONMENT VARIABLES =======================================

services:
  postgres:
    image: postgres:13-alpine
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  init:
    image: *airflow_image
    depends_on:
      - postgres
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username user --password password --firstname Marin --lastname Marin --role Admin --email admin@example.org'

  webserver:
    #    image: *airflow_image
    build:
      context: airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    #        AIRFLOW_CMD: webserver
    restart: always
    env_file:
      - .env
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    volumes:
      - logs:/opt/airflow/logs
    #      - /var/run/docker.sock:/var/run/docker.sock
    #    environment: *airflow_environment
    environment:
      <<: *airflow_environment
      DOCKER_HOST: tcp://socat:2375
    links:
      - socat
    command: webserver

  scheduler:
    #    image: *airflow_image
    build:
      context: airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    #        AIRFLOW_CMD: scheduler
    restart: always
    env_file:
      - .env
    depends_on:
      - postgres
    volumes:
      - logs:/opt/airflow/logs
      - ./dags:/opt/airflow/dags
    #      - /var/run/docker.sock:/var/run/docker.sock
    #    environment: *airflow_environment
    environment:
      <<: *airflow_environment
      DOCKER_HOST: tcp://socat:2375
    links:
      - socat
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    command: scheduler

  insideairbnb:
    build:
      context: insideairbnb/
    ports:
      - "5433:5432"

  #   we are using a local version of s3
  locals3:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=user
      - MINIO_ROOT_PASSWORD=password
    command: server --console-address ":9001" /data
    volumes:
      - "locals3-data:/data"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  locals3_init:
    image: minio/mc
    depends_on:
      - locals3
    entrypoint: >
      /bin/sh -c "
      while ! /usr/bin/mc config host add locals3 http://locals3:9000 user password; do echo 'MinIO not up and running yet...' && sleep 1; done;
      echo 'Added mc host config.';
      /usr/bin/mc mb locals3/data;
      /usr/bin/mc mb locals3/data-backup;
      /usr/bin/mc mb locals3/inside-airbnb;
      exit 0;
      "

  socat:
    image: alpine/socat
    command: tcp-listen:2375,fork,reuseaddr unix-connect:/var/run/docker.sock
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    expose:
      - "2375"

volumes:
  logs:
  locals3-data:
