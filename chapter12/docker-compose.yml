version: '3.8'

# ====================================== AIRFLOW COMMON ========================================

x-airflow-common: &airflow-common
  image: &airflow_image apache/airflow:2.2.3-python3.8
  environment: &airflow-common-env
    AIRFLOW__API__AUTH_BACKEND: airflow.api.auth.backend.basic_auth
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__STORE_DAG_CODE: 'true'
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
  volumes:
    - ./dags:/opt/airflow/dags
    - logs:/opt/airflow/logs
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy

x-vols-dags: &vols-dags
  "./dags:/opt/airflow/dags"

x-vols-logs: &vols-logs
  "logs:/opt/airflow/logs"

# ====================================== /AIRFLOW COMMON =======================================

services:
  postgres:
    image: postgres:14-alpine
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  init:
    image: *airflow_image
    depends_on:
      <<: *airflow-common-depends-on
    environment: *airflow-common-env
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username user --password password --firstname Marin --lastname Marin --role Admin --email admin@example.org'

  webserver:
    <<: *airflow-common
    image: *airflow_image
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test:  [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    environment: *airflow-common-env
    depends_on:
      <<: *airflow-common-depends-on
      init:
        condition: service_completed_successfully

  scheduler:
    <<: *airflow-common
    image: *airflow_image
    command: scheduler
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    environment: *airflow-common-env
    volumes:
      - *vols-dags
      - *vols-logs
    depends_on:
      <<: *airflow-common-depends-on
      init:
        condition: service_completed_successfully


  # docker-compose -f docker-compose.yml up --scale worker=3 -d # from book github repo example to scale instances
  # celery worker
  worker:
    <<: *airflow-common
    image: *airflow_image
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    command: celery worker
    restart: always
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      - scheduler

  redis:
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  # extra dependency for celery
  flower:
    image: *airflow_image
    restart: always
    depends_on:
      - worker
    ports:
      - "5555:5555"
    environment: *airflow-common-env
    command: celery flower

volumes:
  logs:
